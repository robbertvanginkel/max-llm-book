{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSxvy8s5yPsN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCQGyliFxq4U"
      },
      "source": [
        "# Building GPT-2 from Scratch with MAX\n",
        "\n",
        "This interactive notebook guides you through building GPT-2 step by step. Each section corresponds to a step in the tutorial.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's verify MAX is installed and import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJG78Y16xzb7",
        "outputId": "7e4c8487-dda8-443a-e6d1-ed8bbf5579db"
      },
      "outputs": [],
      "source": [
        "!pip install --pre modular \\\n",
        "  --index-url https://dl.modular.com/public/nightly/python/simple/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBmr261yxq4V"
      },
      "outputs": [],
      "source": [
        "# Run this cell to verify MAX is installed and import necessary libraries\n",
        "try:\n",
        "    from dataclasses import dataclass\n",
        "    import math\n",
        "    import numpy as np\n",
        "\n",
        "    from max.driver import CPU, Device\n",
        "    from max.dtype import DType\n",
        "    from max.experimental import functional as F\n",
        "    from max.experimental.tensor import Tensor\n",
        "    from max.graph import Dim, DimLike\n",
        "    from max.nn.module_v3 import Embedding, Linear, Module, Sequential\n",
        "\n",
        "    # Verify MAX is working by creating a simple tensor\n",
        "    test_tensor = Tensor.ones([2, 2], dtype=DType.float32, device=CPU())\n",
        "\n",
        "    print(\"âœ“ MAX is installed and working correctly!\")\n",
        "    print(f\"âœ“ All imports successful\")\n",
        "    print(f\"âœ“ Test tensor created: shape={test_tensor.shape}\")\n",
        "    print(\"\\nYou're ready to start building GPT-2!\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(\"âŒ Error: MAX is not installed or not accessible\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    print(\"\\nTo fix this:\")\n",
        "    print(\"1. Install MAX: https://docs.modular.com/max/\")\n",
        "    print(\"2. Run 'pixi install' from the project root\")\n",
        "    print(\"3. Start Jupyter in the pixi environment: 'pixi run jupyter lab'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dugTNI1Txq4X"
      },
      "source": [
        "## Step 01: Model Configuration\n",
        "\n",
        "Before you can implement GPT-2, you need to define its architecture - the dimensions, layer counts, and structural parameters.\n",
        "\n",
        "Create `GPT2Config`, a class that holds all the architectural decisions for GPT-2.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Add the `@dataclass` decorator to the class\n",
        "2. Get the parameter values from the [GPT-2 config.json](https://huggingface.co/openai-community/gpt2/blob/main/config.json)\n",
        "3. Replace each `None` with the correct value:\n",
        "   - `vocab_size`: 50,257\n",
        "   - `n_positions`: 1,024\n",
        "   - `n_embd`: 768\n",
        "   - `n_layer`: 12\n",
        "   - `n_head`: 12\n",
        "   - `n_inner`: 3,072\n",
        "   - `layer_norm_epsilon`: 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R1C_Wgpxq4X"
      },
      "outputs": [],
      "source": [
        "# TODO: Add @dataclass decorator\n",
        "class GPT2Config:\n",
        "    vocab_size: int = None  # TODO: Replace with correct value\n",
        "    n_positions: int = None  # TODO: Replace with correct value\n",
        "    n_embd: int = None  # TODO: Replace with correct value\n",
        "    n_layer: int = None  # TODO: Replace with correct value\n",
        "    n_head: int = None  # TODO: Replace with correct value\n",
        "    n_inner: int = None  # TODO: Replace with correct value\n",
        "    layer_norm_epsilon: float = None  # TODO: Replace with correct value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPKDrUFpxq4X"
      },
      "outputs": [],
      "source": [
        "# Validation cell - run this to check your configuration\n",
        "config = GPT2Config()\n",
        "assert config.vocab_size == 50257, f\"Expected vocab_size=50257, got {config.vocab_size}\"\n",
        "assert config.n_positions == 1024, f\"Expected n_positions=1024, got {config.n_positions}\"\n",
        "assert config.n_embd == 768, f\"Expected n_embd=768, got {config.n_embd}\"\n",
        "assert config.n_layer == 12, f\"Expected n_layer=12, got {config.n_layer}\"\n",
        "assert config.n_head == 12, f\"Expected n_head=12, got {config.n_head}\"\n",
        "assert config.n_inner == 3072, f\"Expected n_inner=3072, got {config.n_inner}\"\n",
        "assert config.layer_norm_epsilon == 1e-5, f\"Expected layer_norm_epsilon=1e-5, got {config.layer_norm_epsilon}\"\n",
        "print(\"âœ“ Step 01 complete! Configuration is correct.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umdXbT7Uxq4X"
      },
      "source": [
        "## Step 02: Causal Masking\n",
        "\n",
        "Implement the `causal_mask()` function that prevents the model from \"seeing\" future tokens during autoregressive generation.\n",
        "\n",
        "The mask creates a lower triangular pattern where each token can only attend to itself and previous tokens.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Add `@F.functional` decorator to convert the function to a MAX graph operation\n",
        "2. Calculate total sequence length: `n = Dim(sequence_length) + num_tokens`\n",
        "3. Create a `-inf` constant: `Tensor.constant(float(\"-inf\"), dtype=dtype, device=device)`\n",
        "4. Broadcast to target shape: `F.broadcast_to(mask, shape=(sequence_length, n))`\n",
        "5. Apply band_part to create lower triangular pattern: `F.band_part(mask, num_lower=None, num_upper=0, exclude=True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovyw73XYxq4X"
      },
      "outputs": [],
      "source": [
        "# TODO: Add @F.functional decorator\n",
        "def causal_mask(\n",
        "    sequence_length: DimLike,\n",
        "    num_tokens: DimLike,\n",
        "    dtype: DType,\n",
        "    device: Device,\n",
        ") -> Tensor:\n",
        "    # TODO: Calculate total length n\n",
        "    n = None\n",
        "\n",
        "    # TODO: Create -inf constant tensor\n",
        "    mask = None\n",
        "\n",
        "    # TODO: Broadcast to (sequence_length, n) shape\n",
        "    mask = None\n",
        "\n",
        "    # TODO: Apply band_part to create lower triangular pattern\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNeRiehZxq4X"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "test_mask = causal_mask(3, 0, DType.float32, CPU())\n",
        "print(\"Causal mask for sequence length 3:\")\n",
        "print(test_mask)\n",
        "print(\"\\nâœ“ Step 02 complete! Causal mask is working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k3zevIHxq4X"
      },
      "source": [
        "## Step 03: Layer Normalization\n",
        "\n",
        "Create the `LayerNorm` class that normalizes activations across the feature dimension to stabilize training.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Initialize `self.weight` using `Tensor.ones([dim])`\n",
        "2. Initialize `self.bias` using `Tensor.zeros([dim])`\n",
        "3. Apply layer normalization using `F.layer_norm(x, gamma=self.weight, beta=self.bias, epsilon=self.eps)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f8vAz5Xxq4Y"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # TODO: Initialize weight parameter with ones\n",
        "        self.weight = None\n",
        "        # TODO: Initialize bias parameter with zeros\n",
        "        self.bias = None\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: Apply layer normalization\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9bBG7Guxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "ln = LayerNorm(768)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = ln(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"âœ“ Step 03 complete! Layer normalization is working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TdqNKYPxq4Y"
      },
      "source": [
        "## Step 04: Feed-Forward Network (MLP)\n",
        "\n",
        "Build the `MLP` class, a two-layer feed-forward network with GELU activation.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create expansion layer: `Linear(embed_dim, intermediate_size, bias=True)` â†’ `self.c_fc`\n",
        "2. Create projection layer: `Linear(intermediate_size, embed_dim, bias=True)` â†’ `self.c_proj`\n",
        "3. In forward pass:\n",
        "   - Apply expansion: `self.c_fc(hidden_states)`\n",
        "   - Apply GELU: `F.gelu(hidden_states, approximate=\"tanh\")`\n",
        "   - Apply projection: `self.c_proj(hidden_states)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoSeKq46xq4Y"
      },
      "outputs": [],
      "source": [
        "class MLP(Module):\n",
        "    def __init__(self, embed_dim: int, intermediate_size: int):\n",
        "        super().__init__()\n",
        "        # TODO: Create expansion layer (c_fc)\n",
        "        self.c_fc = None\n",
        "        # TODO: Create projection layer (c_proj)\n",
        "        self.c_proj = None\n",
        "\n",
        "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
        "        # TODO: Apply expansion\n",
        "        hidden_states = None\n",
        "        # TODO: Apply GELU activation\n",
        "        hidden_states = None\n",
        "        # TODO: Apply projection and return\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsRVLSMjxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "mlp = MLP(768, 3072)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = mlp(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"âœ“ Step 04 complete! MLP is working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1OrolaOxq4Y"
      },
      "source": [
        "## Step 05: Token Embeddings\n",
        "\n",
        "Create the token embedding layer that converts discrete token IDs into continuous vector representations.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create token embedding layer: `Embedding(config.vocab_size, dim=config.n_embd)` â†’ `self.wte`\n",
        "2. In forward pass: `self.wte(input_ids)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfRF1j9Uxq4Y"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create token embedding layer (wte)\n",
        "        self.wte = None\n",
        "\n",
        "    def forward(self, input_ids: Tensor) -> Tensor:\n",
        "        # TODO: Lookup token embeddings\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "837rgHcsxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "token_emb = TokenEmbedding(config)\n",
        "test_tokens = Tensor([1, 2, 3, 4, 5], dtype=DType.int64, device=CPU())\n",
        "output = token_emb(test_tokens)\n",
        "print(f\"Input tokens shape: {test_tokens.shape}\")\n",
        "print(f\"Output embeddings shape: {output.shape}\")\n",
        "print(\"âœ“ Step 05 complete! Token embeddings working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-7nfhBaxq4Y"
      },
      "source": [
        "## Step 06: Position Embeddings\n",
        "\n",
        "Create position embeddings to encode where each token appears in the sequence.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create position embedding layer: `Embedding(config.n_positions, dim=config.n_embd)` â†’ `self.wpe`\n",
        "2. In forward pass: `self.wpe(position_ids)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgQyOc8zxq4Y"
      },
      "outputs": [],
      "source": [
        "class PositionEmbedding(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create position embedding layer (wpe)\n",
        "        self.wpe = None\n",
        "\n",
        "    def forward(self, position_ids: Tensor) -> Tensor:\n",
        "        # TODO: Lookup position embeddings\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFjAnaCcxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "pos_emb = PositionEmbedding(config)\n",
        "test_positions = Tensor.arange(5, dtype=DType.int64, device=CPU())\n",
        "output = pos_emb(test_positions)\n",
        "print(f\"Input positions shape: {test_positions.shape}\")\n",
        "print(f\"Output embeddings shape: {output.shape}\")\n",
        "print(\"âœ“ Step 06 complete! Position embeddings working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-XN9ilSxq4Y"
      },
      "source": [
        "## Step 07: Q/K/V Projections (Single Head)\n",
        "\n",
        "Implement Q/K/V projections for attention. GPT-2 uses a combined projection layer for efficiency.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create combined Q/K/V projection: `Linear(config.n_embd, 3 * config.n_embd, bias=True)` â†’ `self.c_attn`\n",
        "2. In forward pass:\n",
        "   - Apply projection: `qkv = self.c_attn(x)`\n",
        "   - Split into Q, K, V: `F.split(qkv, [self.n_embd, self.n_embd, self.n_embd], axis=-1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNedEeoGxq4Y"
      },
      "outputs": [],
      "source": [
        "class QKVProjection(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        # TODO: Create combined Q/K/V projection layer (c_attn)\n",
        "        self.c_attn = None\n",
        "\n",
        "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        # TODO: Apply combined projection\n",
        "        qkv = None\n",
        "        # TODO: Split into Q, K, V\n",
        "        query, key, value = None, None, None\n",
        "        return query, key, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaZiXjbcxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "qkv_proj = QKVProjection(config)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "q, k, v = qkv_proj(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Query shape: {q.shape}\")\n",
        "print(f\"Key shape: {k.shape}\")\n",
        "print(f\"Value shape: {v.shape}\")\n",
        "print(\"âœ“ Step 07 complete! Q/K/V projections working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXqcBt7kxq4Z"
      },
      "source": [
        "## Step 08: Attention Mechanism with Causal Masking\n",
        "\n",
        "Implement the core attention mechanism using scaled dot-product attention.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Compute attention scores: `query @ key.transpose(-1, -2)`\n",
        "2. Scale scores: divide by `math.sqrt(int(value.shape[-1]))`\n",
        "3. Apply causal mask: `attn_weights + causal_mask(...)`\n",
        "4. Apply softmax: `F.softmax(attn_weights)`\n",
        "5. Compute output: `attn_weights @ value`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22UNTEmZxq4Z"
      },
      "outputs": [],
      "source": [
        "def compute_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "    # TODO: Compute attention scores (Q @ K^T)\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Scale by sqrt(d_k)\n",
        "    scale_factor = None\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Apply causal mask\n",
        "    seq_len = query.shape[-2]\n",
        "    mask = causal_mask(seq_len, 0, dtype=query.dtype, device=query.device)\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Apply softmax\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Compute weighted sum of values\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS2I_zrDxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "test_q = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "test_k = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "test_v = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = compute_attention(test_q, test_k, test_v)\n",
        "print(f\"Query shape: {test_q.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"âœ“ Step 08 complete! Attention mechanism working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQMFe5vExq4Z"
      },
      "source": [
        "## Step 09: Multi-Head Attention\n",
        "\n",
        "Extend single-head attention to multi-head attention by splitting embeddings across 12 heads.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create Q/K/V projection: `Linear(config.n_embd, 3 * config.n_embd, bias=True)` â†’ `self.c_attn`\n",
        "2. Create output projection: `Linear(config.n_embd, config.n_embd, bias=True)` â†’ `self.c_proj`\n",
        "3. Implement `_split_heads`: reshape and transpose to `[batch, num_heads, seq_length, head_dim]`\n",
        "4. Implement `_merge_heads`: reverse the splitting operation\n",
        "5. In forward: project â†’ split heads â†’ attention â†’ merge heads â†’ output projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU2XiVx9xq4Z"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.n_head\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.split_size = config.n_embd\n",
        "\n",
        "        # TODO: Create combined Q/K/V projection\n",
        "        self.c_attn = None\n",
        "        # TODO: Create output projection\n",
        "        self.c_proj = None\n",
        "\n",
        "    def _split_heads(self, tensor: Tensor, num_heads: int, attn_head_size: int) -> Tensor:\n",
        "        # TODO: Reshape to add head dimension\n",
        "        new_shape = tensor.shape[:-1] + [num_heads, attn_head_size]\n",
        "        tensor = None\n",
        "        # TODO: Transpose to move heads to position 1\n",
        "        return None\n",
        "\n",
        "    def _merge_heads(self, tensor: Tensor, num_heads: int, attn_head_size: int) -> Tensor:\n",
        "        # TODO: Transpose heads back\n",
        "        tensor = None\n",
        "        # TODO: Reshape to flatten heads\n",
        "        new_shape = tensor.shape[:-2] + [num_heads * attn_head_size]\n",
        "        return None\n",
        "\n",
        "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
        "        # TODO: Project to Q/K/V and split\n",
        "        qkv = None\n",
        "        query, key, value = None, None, None\n",
        "\n",
        "        # TODO: Split heads for Q, K, V\n",
        "        query = None\n",
        "        key = None\n",
        "        value = None\n",
        "\n",
        "        # TODO: Compute attention\n",
        "        attn_output = None\n",
        "\n",
        "        # TODO: Merge heads\n",
        "        attn_output = None\n",
        "\n",
        "        # TODO: Apply output projection\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCxvQg1Mxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "mha = MultiHeadAttention(config)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = mha(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"âœ“ Step 09 complete! Multi-head attention working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYdk9SUXxq4Z"
      },
      "source": [
        "## Step 10: Residual Connections and Layer Normalization Pattern\n",
        "\n",
        "The LayerNorm class is already implemented in Step 03. This step demonstrates the pre-norm pattern used in GPT-2.\n",
        "\n",
        "The pattern is: `x = x + sublayer(layer_norm(x))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fHtKsN7xq4Z"
      },
      "outputs": [],
      "source": [
        "# Demonstration of pre-norm pattern\n",
        "def apply_with_residual(x: Tensor, layer_norm: LayerNorm, sublayer: Module) -> Tensor:\n",
        "    \"\"\"Apply pre-norm pattern: x = x + sublayer(layer_norm(x))\"\"\"\n",
        "    return x + sublayer(layer_norm(x))\n",
        "\n",
        "# Validation\n",
        "ln = LayerNorm(768)\n",
        "mlp_layer = MLP(768, 3072)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = apply_with_residual(test_input, ln, mlp_layer)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"âœ“ Step 10 complete! Pre-norm pattern demonstrated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRz6soQhxq4Z"
      },
      "source": [
        "## Step 11: Transformer Block\n",
        "\n",
        "Combine attention, MLP, layer normalization, and residual connections into a complete transformer block.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create first layer norm: `LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)` â†’ `self.ln_1`\n",
        "2. Create attention: `MultiHeadAttention(config)` â†’ `self.attn`\n",
        "3. Create second layer norm: `LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)` â†’ `self.ln_2`\n",
        "4. Create MLP: `MLP(config.n_embd, config.n_inner)` â†’ `self.mlp`\n",
        "5. In forward:\n",
        "   - `x = x + self.attn(self.ln_1(x))`\n",
        "   - `x = x + self.mlp(self.ln_2(x))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lp8zS2Qxq4Z"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create first layer norm (ln_1)\n",
        "        self.ln_1 = None\n",
        "        # TODO: Create multi-head attention (attn)\n",
        "        self.attn = None\n",
        "        # TODO: Create second layer norm (ln_2)\n",
        "        self.ln_2 = None\n",
        "        # TODO: Create MLP (mlp)\n",
        "        self.mlp = None\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: Apply attention with pre-norm and residual\n",
        "        x = None\n",
        "        # TODO: Apply MLP with pre-norm and residual\n",
        "        x = None\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmYZgvYJxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "block = TransformerBlock(config)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = block(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"âœ“ Step 11 complete! Transformer block working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWH6jzc3xq4Z"
      },
      "source": [
        "## Step 12: Stacking Transformer Blocks\n",
        "\n",
        "Stack 12 transformer blocks with embeddings and final normalization to create the complete GPT-2 model.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create token embeddings: `Embedding(config.vocab_size, dim=config.n_embd)` â†’ `self.wte`\n",
        "2. Create position embeddings: `Embedding(config.n_positions, dim=config.n_embd)` â†’ `self.wpe`\n",
        "3. Stack transformer blocks: `Sequential(*[TransformerBlock(config) for _ in range(config.n_layer)])` â†’ `self.h`\n",
        "4. Create final layer norm: `LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)` â†’ `self.ln_f`\n",
        "5. In forward:\n",
        "   - Get sequence length and create positions\n",
        "   - Combine token and position embeddings\n",
        "   - Pass through transformer blocks\n",
        "   - Apply final layer norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYtVFS_mxq4Z"
      },
      "outputs": [],
      "source": [
        "class GPT2Model(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create token embeddings (wte)\n",
        "        self.wte = None\n",
        "        # TODO: Create position embeddings (wpe)\n",
        "        self.wpe = None\n",
        "        # TODO: Stack transformer blocks (h)\n",
        "        self.h = None\n",
        "        # TODO: Create final layer norm (ln_f)\n",
        "        self.ln_f = None\n",
        "\n",
        "    def forward(self, input_ids: Tensor) -> Tensor:\n",
        "        # TODO: Get sequence length\n",
        "        seq_length = None\n",
        "\n",
        "        # TODO: Create position indices\n",
        "        positions = None\n",
        "\n",
        "        # TODO: Get token and position embeddings\n",
        "        token_embeds = None\n",
        "        pos_embeds = None\n",
        "\n",
        "        # TODO: Combine embeddings\n",
        "        hidden_states = None\n",
        "\n",
        "        # TODO: Pass through transformer blocks\n",
        "        hidden_states = None\n",
        "\n",
        "        # TODO: Apply final layer norm\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y7_IWgAxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "model = GPT2Model(config)\n",
        "test_tokens = Tensor([[1, 2, 3, 4, 5]], dtype=DType.int64, device=CPU())\n",
        "output = model(test_tokens)\n",
        "print(f\"Input tokens shape: {test_tokens.shape}\")\n",
        "print(f\"Output hidden states shape: {output.shape}\")\n",
        "print(\"âœ“ Step 12 complete! GPT-2 model working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24TpwEJvxq4Z"
      },
      "source": [
        "## Step 13: Language Model Head\n",
        "\n",
        "Add the final linear projection layer that converts hidden states to vocabulary logits.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create transformer: `GPT2Model(config)` â†’ `self.transformer`\n",
        "2. Create LM head: `Linear(config.n_embd, config.vocab_size, bias=False)` â†’ `self.lm_head`\n",
        "3. In forward:\n",
        "   - Get hidden states from transformer\n",
        "   - Project to logits with LM head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Futk0uPKxq4i"
      },
      "outputs": [],
      "source": [
        "class GPT2LMHeadModel(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create transformer\n",
        "        self.transformer = None\n",
        "        # TODO: Create LM head (note: bias=False)\n",
        "        self.lm_head = None\n",
        "\n",
        "    def forward(self, input_ids: Tensor) -> Tensor:\n",
        "        # TODO: Get hidden states from transformer\n",
        "        hidden_states = None\n",
        "        # TODO: Project to vocabulary logits\n",
        "        logits = None\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUzuuyJtxq4i"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "lm_model = GPT2LMHeadModel(config)\n",
        "test_tokens = Tensor([[1, 2, 3, 4, 5]], dtype=DType.int64, device=CPU())\n",
        "logits = lm_model(test_tokens)\n",
        "print(f\"Input tokens shape: {test_tokens.shape}\")\n",
        "print(f\"Output logits shape: {logits.shape}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(\"âœ“ Step 13 complete! Language model head working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMQORLmBxq4i"
      },
      "source": [
        "## Step 14: Text Generation\n",
        "\n",
        "Implement text generation with temperature and sampling.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Implement `generate_next_token`:\n",
        "   - Get logits from model\n",
        "   - Extract last position logits\n",
        "   - Apply temperature scaling\n",
        "   - Sample from probability distribution or use greedy decoding\n",
        "2. Implement `generate`:\n",
        "   - Loop for max_new_tokens iterations\n",
        "   - Generate next token\n",
        "   - Concatenate to sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diEb8li7xq4i"
      },
      "outputs": [],
      "source": [
        "def generate_next_token(\n",
        "    model: GPT2LMHeadModel,\n",
        "    input_ids: Tensor,\n",
        "    temperature: float = 1.0,\n",
        "    do_sample: bool = True,\n",
        ") -> Tensor:\n",
        "    # TODO: Get logits from model\n",
        "    logits = None\n",
        "\n",
        "    # TODO: Extract last position logits\n",
        "    next_token_logits = None\n",
        "\n",
        "    if do_sample:\n",
        "        # TODO: Create temperature tensor\n",
        "        temp_tensor = None\n",
        "        # TODO: Scale logits by temperature\n",
        "        next_token_logits = None\n",
        "\n",
        "        # TODO: Convert to probabilities\n",
        "        probs = None\n",
        "\n",
        "        # TODO: Transfer to CPU and convert to NumPy\n",
        "        probs_np = None\n",
        "\n",
        "        # TODO: Sample from distribution\n",
        "        next_token_id = None\n",
        "\n",
        "        # TODO: Convert back to tensor\n",
        "        next_token_tensor = None\n",
        "    else:\n",
        "        # TODO: Greedy decoding (argmax)\n",
        "        next_token_tensor = None\n",
        "\n",
        "    return next_token_tensor\n",
        "\n",
        "def generate(\n",
        "    model: GPT2LMHeadModel,\n",
        "    input_ids: Tensor,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float = 1.0,\n",
        "    do_sample: bool = True,\n",
        ") -> Tensor:\n",
        "    # TODO: Initialize with input tokens\n",
        "    generated_tokens = input_ids\n",
        "\n",
        "    # TODO: Generate loop\n",
        "    for _ in range(max_new_tokens):\n",
        "        # TODO: Generate next token\n",
        "        next_token = None\n",
        "\n",
        "        # TODO: Reshape to 2D\n",
        "        next_token_2d = None\n",
        "\n",
        "        # TODO: Concatenate to sequence\n",
        "        generated_tokens = None\n",
        "\n",
        "    return generated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrhwvxBHxq4j"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "# Note: This will generate random tokens since we haven't loaded pretrained weights\n",
        "prompt = Tensor([[15496, 995]], dtype=DType.int64, device=CPU())  # \"Hello world\"\n",
        "generated = generate(lm_model, prompt, max_new_tokens=5, temperature=1.0, do_sample=False)\n",
        "print(f\"Prompt shape: {prompt.shape}\")\n",
        "print(f\"Generated sequence shape: {generated.shape}\")\n",
        "print(f\"Generated token IDs: {generated}\")\n",
        "print(\"\\nâœ“ Step 14 complete! Text generation working.\")\n",
        "print(\"\\nðŸŽ‰ Congratulations! You've built GPT-2 from scratch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tRs3EBsxq4j"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You've completed all 14 steps and built a complete GPT-2 model from scratch using MAX!\n",
        "\n",
        "### What you've built:\n",
        "\n",
        "1. **Model configuration** - Architecture parameters\n",
        "2. **Causal masking** - Prevents attending to future tokens\n",
        "3. **Layer normalization** - Stabilizes training\n",
        "4. **Feed-forward network (MLP)** - Adds non-linearity\n",
        "5. **Token embeddings** - Converts token IDs to vectors\n",
        "6. **Position embeddings** - Encodes sequence order\n",
        "7. **Q/K/V projections** - Prepares inputs for attention\n",
        "8. **Attention mechanism** - Core attention computation\n",
        "9. **Multi-head attention** - Parallel attention heads\n",
        "10. **Residual connections** - Enables deep networks\n",
        "11. **Transformer block** - Complete repeating unit\n",
        "12. **Model stacking** - Combines all components\n",
        "13. **Language model head** - Projects to vocabulary\n",
        "14. **Text generation** - Autoregressive sampling\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
